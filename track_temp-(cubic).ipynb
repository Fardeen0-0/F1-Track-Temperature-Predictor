{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4484972-6342-4902-b4e6-1d388412f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41381630-d38b-4511-9e9a-0ef8fec47f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "# weather.csv contains F1 weather data from 2018 - 2023\n",
    "\n",
    "feature_categories = [1,2,3,4,6,7]\n",
    "features = pd.read_csv(\"weather.csv\", usecols=feature_categories)\n",
    "target = pd.read_csv(\"weather.csv\", usecols=['TrackTemp'])\n",
    "test_features = pd.read_csv(\"weather_2024.csv\", usecols=feature_categories)\n",
    "test_target = pd.read_csv(\"weather_2024.csv\", usecols=['TrackTemp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b620af-ba6e-4bdf-9ac0-a1f6a1593ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AirTemp  Humidity  Pressure  Rainfall  WindDirection  WindSpeed\n",
      "0     24.1      36.2     997.1     False            294        3.0\n",
      "1     24.0      36.3     997.1     False            273        1.4\n",
      "2     24.0      36.3     997.1     False            273        1.4\n",
      "3     23.9      37.2     997.0     False            287        2.3\n",
      "4     24.2      35.8     997.1     False            309        3.5\n",
      "   TrackTemp\n",
      "0       38.2\n",
      "1       38.6\n",
      "2       38.6\n",
      "3       38.7\n",
      "4       38.7\n"
     ]
    }
   ],
   "source": [
    "# Checking if data loaded\n",
    "\n",
    "print(features.head())\n",
    "print(target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b887d4f4-4c39-4f27-933c-dee50668f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting pandas format to numpy / Assigning m & n\n",
    "\n",
    "x_train = features.to_numpy()\n",
    "# x_train[:,3] = x_train[:,3].astype(float) # converts the type of the entire column\n",
    "# x_train[:,4] = x_train[:,4].astype(float)\n",
    "x_train = x_train.astype(float)\n",
    "x_train[:,1] = np.round(x_train[:,1])   #Just to convert humidity to whole number cuz testing data consists of integers\n",
    "y_train = target.to_numpy()\n",
    "\n",
    "x_test_raw = test_features.to_numpy()\n",
    "x_test_raw = x_test_raw.astype(float)\n",
    "y_test = test_target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ccde54-abcb-41c7-98f8-54542acc62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "(18214, 6)\n",
      "(18214, 1)\n",
      "[[38.2]\n",
      " [38.6]\n",
      " [38.6]\n",
      " [38.7]\n",
      " [38.7]]\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "(25668, 6)\n",
      "(25668, 1)\n",
      "[[23.8]\n",
      " [23.8]\n",
      " [23.8]\n",
      " [23.7]\n",
      " [23.5]]\n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(x_train.dtype)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[:5])\n",
    "print()\n",
    "print(type(x_test_raw))\n",
    "print(type(y_test))\n",
    "print(x_test_raw.dtype)\n",
    "print(x_test_raw.shape)\n",
    "print(y_test.shape)\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87be782d-d556-44b7-b159-326c928c1767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "-------------\n",
      "Mean: 35.36093115186121\n",
      "Std Dev: 9.443176757529569\n",
      "Min: 13.8\n",
      "Max: 67.0\n",
      "\n",
      "Testing Data\n",
      "------------\n",
      "Mean: 36.24066152407667\n",
      "Std Dev: 8.477747117055532\n",
      "Min: 19.9\n",
      "Max: 51.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data\")\n",
    "print(\"-------------\")\n",
    "print(\"Mean:\", np.mean(y_train))\n",
    "print(\"Std Dev:\", np.std(y_train))\n",
    "print(\"Min:\", np.min(y_train))\n",
    "print(\"Max:\", np.max(y_train))\n",
    "print()\n",
    "print(\"Testing Data\")\n",
    "print(\"------------\")\n",
    "print(\"Mean:\", np.mean(y_test))\n",
    "print(\"Std Dev:\", np.std(y_test))\n",
    "print(\"Min:\", np.min(y_test))\n",
    "print(\"Max:\", np.max(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a7afa0-fa5b-4d64-bde0-3e425263a0aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Failed attempts at appending x_train with interaction terms for quadratic regression\n",
    "\n",
    "# x = []\n",
    "# for i in range(len(x_train)):\n",
    "#     for j in range(len(x_train)):\n",
    "#         x.append(x_train[i]*x_train[j])\n",
    "# x_train.extend(x)\n",
    "# ^ Wrong ^ Multiplies the entire row with another row. Nah, u gotta do individual elements mate.\n",
    "\n",
    "# for i in range(m):\n",
    "#     x = []\n",
    "#     for j in range(n):\n",
    "#         for k in range(j,n):\n",
    "#             x.append(x_train[i,j]*x_train[i,k])\n",
    "#     x_train[i].extend(x)\n",
    "# ^ Also wrong ^ Numpy arrays don't have extend() and also they are fixed size: you can't change the length of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d9e2566-754c-416d-a3a6-7d3d29c5cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Appending x_train with interaction terms for quadratic regression\n",
    "\n",
    "# def add_interaction_terms(x):\n",
    "#     m,n = x.shape\n",
    "#     new_x = []\n",
    "#     for i in range(m):\n",
    "#         temp = []\n",
    "#         for j in range(n):\n",
    "#             for k in range(j,n):\n",
    "#                 temp.append(x[i,j]*x[i,k])\n",
    "#         row = np.concatenate([x[i],np.array(temp)])\n",
    "#         new_x.append(row)\n",
    "\n",
    "# #     # m,n = x.shape\n",
    "# #     # new_x = []\n",
    "# #     # for i in range(m):\n",
    "# #     #     temp = np.dot(x[i],x[i])\n",
    "# #     #     row = np.concatenate([x[i],np.array(temp)])\n",
    "# #     #     new_x.append(row)\n",
    "        \n",
    "#     return np.array(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd151fd-4fca-4dd7-b93e-fec146b86e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending x_train with interaction terms for cubic regression\n",
    "\n",
    "def add_interaction_terms(x):\n",
    "    m,n = x.shape\n",
    "    rows = []\n",
    "    for i in range(m):\n",
    "        base = list(x[i])  # linear terms\n",
    "\n",
    "        # quadratic terms (including squares)\n",
    "        quad = []\n",
    "        for j in range(n):\n",
    "            for k in range(j, n):  # j ≤ k\n",
    "                quad.append(x[i, j] * x[i, k])\n",
    "\n",
    "        # cubic terms\n",
    "        cubic = []\n",
    "        for j in range(n):\n",
    "            for k in range(j, n):          # j ≤ k\n",
    "                for l in range(k, n):      # k ≤ l\n",
    "                    cubic.append(x[i, j] * x[i, k] * x[i, l])\n",
    "\n",
    "        rows.append(np.array(base + quad + cubic, dtype=float))\n",
    "\n",
    "    return np.vstack(rows)   #Formats it as a 2D numpy array\n",
    "\n",
    "# Pretty neat this GPT dude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ba250c-a641-40be-b8c3-407a536bac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18214, 83)\n"
     ]
    }
   ],
   "source": [
    "x_train = add_interaction_terms(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a017c6c-7cc3-4af0-8215-0ad66cad149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning m and n\n",
    "\n",
    "m = x_train.shape[0]\n",
    "n = x_train.shape[1]\n",
    "#m,n = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0a5d47-bc75-42d1-8b6a-577ed84da999",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Attempts at defining Z-score normalization (standardization)\n",
    "\n",
    "# Do I do all in one function or split it into three functions and use the two in z-score? Both work\n",
    "# def z_score(x):\n",
    "    \n",
    "#     mu = np.zeros(n)\n",
    "#     for j in range(n):\n",
    "#         sum = 0\n",
    "#         for i in range(m):\n",
    "#             sum += x[i][j]\n",
    "#         mu[j] = sum/m\n",
    "        \n",
    "#     sigma = np.zeros(n)\n",
    "#     for j in range(n):\n",
    "#         sum = 0\n",
    "#         for i in range(m):\n",
    "#             sum += ((x[i][j] - mu[j])**2)\n",
    "#         sigma[j] = (sum/m)**0.5\n",
    "\n",
    "#     for j in range(n):\n",
    "#         for i in range(m):\n",
    "#             x[i][j] = (x[i][j] - mu[j])/sigma[j]\n",
    "# ----------------------------------------------------------------------------------\n",
    "# def mean_array(x):\n",
    "#     mu = np.zeros(n)\n",
    "#     for j in range(n):\n",
    "#         sum = 0\n",
    "#         for i in range(m):\n",
    "#             sum += x[i][j]\n",
    "#         mu[j] = sum/m\n",
    "#     return mu\n",
    "\n",
    "# def std_dev(x,mu):\n",
    "#     sigma = np.zeros(n)\n",
    "#     for j in range(n):\n",
    "#         sum = 0\n",
    "#         for i in range(m):\n",
    "#             sum += ((x[i][j] - mu[j])**2)\n",
    "#         sigma[j] = (sum/m)**0.5\n",
    "#     return sigma\n",
    "# def z_score(x):\n",
    "#     mu = mean_array(x)\n",
    "#     sigma = std_dev(x,mu)\n",
    "#     for j in range(n):\n",
    "#         for i in range(m):\n",
    "#             x[i][j] = (x[i][j] - mu[j])/sigma[j]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e8234f-82e7-4165-9b48-f4a279937e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Z-score normalization (Better and concise way using numpy)\n",
    "    \n",
    "# def z_score(x):\n",
    "#     mu = np.mean(x, axis=0)\n",
    "#     sigma = np.std(x, axis=0)\n",
    "#     for j in range(n):\n",
    "#         for i in range(m):\n",
    "#             x[i][j] = (x[i][j] - mu[j]) / sigma[j]\n",
    "\n",
    "# Or even better:\n",
    "def z_score(x):\n",
    "    mu = np.mean(x, axis=0) \n",
    "    sigma = np.std(x, axis=0)\n",
    "    sigma_safe = np.where(sigma==0,1,sigma) # Replaces sigma=0 with 1. If not sigma!=0, replaces with sigma(itself) (i.e. nothing happens)\n",
    "    # mu = np.nanmean(x, axis=0)   # Ignore NaNs for mean # axis=0 means operate column-wise. Go down the rows for each column\n",
    "    # sigma = np.nanstd(x, axis=0) # Ignore NaNs for std\n",
    "    # sigma[sigma == 0] = 1  # Prevent division by zero\n",
    "\n",
    "    x[:] = (x - mu) / sigma_safe # The [:] accesses each element within x and updates it in-place (pretty cool ngl; without all that for i for j stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be9e6338-49c6-4796-b6a8-b26012f1dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[2.41000000e+01 3.60000000e+01 9.97100000e+02 0.00000000e+00\n",
      "  2.94000000e+02 3.00000000e+00 5.80810000e+02 8.67600000e+02\n",
      "  2.40301100e+04 0.00000000e+00 7.08540000e+03 7.23000000e+01\n",
      "  1.29600000e+03 3.58956000e+04 0.00000000e+00 1.05840000e+04\n",
      "  1.08000000e+02 9.94208410e+05 0.00000000e+00 2.93147400e+05\n",
      "  2.99130000e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  8.64360000e+04 8.82000000e+02 9.00000000e+00 1.39975210e+04\n",
      "  2.09091600e+04 5.79125651e+05 0.00000000e+00 1.70758140e+05\n",
      "  1.74243000e+03 3.12336000e+04 8.65083960e+05 0.00000000e+00\n",
      "  2.55074400e+05 2.60280000e+03 2.39604227e+07 0.00000000e+00\n",
      "  7.06485234e+06 7.20903300e+04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.08310760e+06 2.12562000e+04 2.16900000e+02\n",
      "  4.66560000e+04 1.29224160e+06 0.00000000e+00 3.81024000e+05\n",
      "  3.88800000e+03 3.57915028e+07 0.00000000e+00 1.05533064e+07\n",
      "  1.07686800e+05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.11169600e+06 3.17520000e+04 3.24000000e+02 9.91325206e+08\n",
      "  0.00000000e+00 2.92297273e+08 2.98262523e+06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.61853356e+07 8.79442200e+05\n",
      "  8.97390000e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.54121840e+07\n",
      "  2.59308000e+05 2.64600000e+03 2.70000000e+01]\n",
      " [2.40000000e+01 3.60000000e+01 9.97100000e+02 0.00000000e+00\n",
      "  2.73000000e+02 1.40000000e+00 5.76000000e+02 8.64000000e+02\n",
      "  2.39304000e+04 0.00000000e+00 6.55200000e+03 3.36000000e+01\n",
      "  1.29600000e+03 3.58956000e+04 0.00000000e+00 9.82800000e+03\n",
      "  5.04000000e+01 9.94208410e+05 0.00000000e+00 2.72208300e+05\n",
      "  1.39594000e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  7.45290000e+04 3.82200000e+02 1.96000000e+00 1.38240000e+04\n",
      "  2.07360000e+04 5.74329600e+05 0.00000000e+00 1.57248000e+05\n",
      "  8.06400000e+02 3.11040000e+04 8.61494400e+05 0.00000000e+00\n",
      "  2.35872000e+05 1.20960000e+03 2.38610018e+07 0.00000000e+00\n",
      "  6.53299920e+06 3.35025600e+04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.78869600e+06 9.17280000e+03 4.70400000e+01\n",
      "  4.66560000e+04 1.29224160e+06 0.00000000e+00 3.53808000e+05\n",
      "  1.81440000e+03 3.57915028e+07 0.00000000e+00 9.79949880e+06\n",
      "  5.02538400e+04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.68304400e+06 1.37592000e+04 7.05600000e+01 9.91325206e+08\n",
      "  0.00000000e+00 2.71418896e+08 1.39189177e+06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 7.43128659e+07 3.81091620e+05\n",
      "  1.95431600e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.03464170e+07\n",
      "  1.04340600e+05 5.35080000e+02 2.74400000e+00]\n",
      " [2.40000000e+01 3.60000000e+01 9.97100000e+02 0.00000000e+00\n",
      "  2.73000000e+02 1.40000000e+00 5.76000000e+02 8.64000000e+02\n",
      "  2.39304000e+04 0.00000000e+00 6.55200000e+03 3.36000000e+01\n",
      "  1.29600000e+03 3.58956000e+04 0.00000000e+00 9.82800000e+03\n",
      "  5.04000000e+01 9.94208410e+05 0.00000000e+00 2.72208300e+05\n",
      "  1.39594000e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  7.45290000e+04 3.82200000e+02 1.96000000e+00 1.38240000e+04\n",
      "  2.07360000e+04 5.74329600e+05 0.00000000e+00 1.57248000e+05\n",
      "  8.06400000e+02 3.11040000e+04 8.61494400e+05 0.00000000e+00\n",
      "  2.35872000e+05 1.20960000e+03 2.38610018e+07 0.00000000e+00\n",
      "  6.53299920e+06 3.35025600e+04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.78869600e+06 9.17280000e+03 4.70400000e+01\n",
      "  4.66560000e+04 1.29224160e+06 0.00000000e+00 3.53808000e+05\n",
      "  1.81440000e+03 3.57915028e+07 0.00000000e+00 9.79949880e+06\n",
      "  5.02538400e+04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.68304400e+06 1.37592000e+04 7.05600000e+01 9.91325206e+08\n",
      "  0.00000000e+00 2.71418896e+08 1.39189177e+06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 7.43128659e+07 3.81091620e+05\n",
      "  1.95431600e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.03464170e+07\n",
      "  1.04340600e+05 5.35080000e+02 2.74400000e+00]\n",
      " [2.39000000e+01 3.70000000e+01 9.97000000e+02 0.00000000e+00\n",
      "  2.87000000e+02 2.30000000e+00 5.71210000e+02 8.84300000e+02\n",
      "  2.38283000e+04 0.00000000e+00 6.85930000e+03 5.49700000e+01\n",
      "  1.36900000e+03 3.68890000e+04 0.00000000e+00 1.06190000e+04\n",
      "  8.51000000e+01 9.94009000e+05 0.00000000e+00 2.86139000e+05\n",
      "  2.29310000e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  8.23690000e+04 6.60100000e+02 5.29000000e+00 1.36519190e+04\n",
      "  2.11347700e+04 5.69496370e+05 0.00000000e+00 1.63937270e+05\n",
      "  1.31378300e+03 3.27191000e+04 8.81647100e+05 0.00000000e+00\n",
      "  2.53794100e+05 2.03389000e+03 2.37568151e+07 0.00000000e+00\n",
      "  6.83872210e+06 5.48050900e+04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.96861910e+06 1.57763900e+04 1.26431000e+02\n",
      "  5.06530000e+04 1.36489300e+06 0.00000000e+00 3.92903000e+05\n",
      "  3.14870000e+03 3.67783330e+07 0.00000000e+00 1.05871430e+07\n",
      "  8.48447000e+04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.04765300e+06 2.44237000e+04 1.95730000e+02 9.91026973e+08\n",
      "  0.00000000e+00 2.85280583e+08 2.28622070e+06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.21218930e+07 6.58119700e+05\n",
      "  5.27413000e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.36399030e+07\n",
      "  1.89448700e+05 1.51823000e+03 1.21670000e+01]\n",
      " [2.42000000e+01 3.60000000e+01 9.97100000e+02 0.00000000e+00\n",
      "  3.09000000e+02 3.50000000e+00 5.85640000e+02 8.71200000e+02\n",
      "  2.41298200e+04 0.00000000e+00 7.47780000e+03 8.47000000e+01\n",
      "  1.29600000e+03 3.58956000e+04 0.00000000e+00 1.11240000e+04\n",
      "  1.26000000e+02 9.94208410e+05 0.00000000e+00 3.08103900e+05\n",
      "  3.48985000e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  9.54810000e+04 1.08150000e+03 1.22500000e+01 1.41724880e+04\n",
      "  2.10830400e+04 5.83941644e+05 0.00000000e+00 1.80962760e+05\n",
      "  2.04974000e+03 3.13632000e+04 8.68673520e+05 0.00000000e+00\n",
      "  2.69200800e+05 3.04920000e+03 2.40598435e+07 0.00000000e+00\n",
      "  7.45611438e+06 8.44543700e+04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.31064020e+06 2.61723000e+04 2.96450000e+02\n",
      "  4.66560000e+04 1.29224160e+06 0.00000000e+00 4.00464000e+05\n",
      "  4.53600000e+03 3.57915028e+07 0.00000000e+00 1.10917404e+07\n",
      "  1.25634600e+05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.43731600e+06 3.89340000e+04 4.41000000e+02 9.91325206e+08\n",
      "  0.00000000e+00 3.07210399e+08 3.47972944e+06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 9.52041051e+07 1.07836365e+06\n",
      "  1.22144750e+04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.95036290e+07\n",
      "  3.34183500e+05 3.78525000e+03 4.28750000e+01]]\n",
      "[[ 9.55222332e-02 -1.02139832e+00  2.16702584e-01 -2.82481970e-01\n",
      "   1.15381088e+00  1.18050847e+00 -9.94423179e-03 -9.04999383e-01\n",
      "   1.40796864e-01 -2.72775869e-01  1.14221188e+00  1.11890703e+00\n",
      "  -9.83897521e-01 -9.73845743e-01 -2.76486517e-01  1.43698066e-01\n",
      "   3.39253997e-01  2.07657793e-01 -2.82436282e-01  1.17771516e+00\n",
      "   1.18543530e+00 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "   1.18426530e+00  2.01563964e+00  8.23473465e-01 -1.07969472e-01\n",
      "  -6.69899987e-01  1.46788653e-02 -2.50596845e-01  9.25976018e-01\n",
      "   8.95173534e-01 -9.97218308e-01 -8.54958444e-01 -2.72613642e-01\n",
      "   2.41430359e-01  4.03251245e-01  1.66542774e-01 -2.72554353e-01\n",
      "   1.15898774e+00  1.12449111e+00 -2.72775869e-01 -2.41189285e-01\n",
      "  -2.14023406e-01  1.21382744e+00  2.05090820e+00  7.77090102e-01\n",
      "  -8.71419185e-01 -9.70897416e-01 -2.64101662e-01 -3.55552669e-01\n",
      "  -2.40401667e-01 -9.28465377e-01 -2.76795928e-01  1.52667878e-01\n",
      "   3.43136503e-01 -2.76486517e-01 -2.31869318e-01 -2.18014247e-01\n",
      "   3.43247741e-01  9.46499928e-01  3.62716591e-01  1.97334745e-01\n",
      "  -2.82301615e-01  1.18353856e+00  1.18374008e+00 -2.82436282e-01\n",
      "  -2.42155731e-01 -2.18422711e-01  1.20191619e+00  2.01401864e+00\n",
      "   8.23571977e-01 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "  -2.16846891e-01 -1.87066926e-01 -1.53955194e-01  1.09675434e+00\n",
      "   2.10725776e+00  1.49124562e+00  4.04051461e-01]\n",
      " [ 7.56080555e-02 -1.02139832e+00  2.16702584e-01 -2.82481970e-01\n",
      "   9.51372985e-01 -1.85285591e-01 -3.05085509e-02 -9.13550989e-01\n",
      "   1.21485047e-01 -2.72775869e-01  9.37176205e-01 -1.60052500e-01\n",
      "  -9.83897521e-01 -9.73845743e-01 -2.76486517e-01  3.54203568e-02\n",
      "  -5.25742815e-01  2.07657793e-01 -2.82436282e-01  9.74503276e-01\n",
      "  -1.73682360e-01 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "   8.72924624e-01  3.12718157e-01 -3.33026302e-01 -1.27939880e-01\n",
      "  -6.82620797e-01 -5.71284869e-03 -2.50596845e-01  7.45250694e-01\n",
      "  -1.68712050e-01 -1.00026769e+00 -8.63211955e-01 -2.72613642e-01\n",
      "   1.14914361e-01 -4.86648361e-01  1.48370403e-01 -2.72554353e-01\n",
      "   9.53995173e-01 -1.49953687e-01 -2.72775869e-01 -2.41189285e-01\n",
      "  -2.14023406e-01  8.94175180e-01  3.27537540e-01 -3.11093464e-01\n",
      "  -8.71419185e-01 -9.70897416e-01 -2.64101662e-01 -4.02686649e-01\n",
      "  -6.93399191e-01 -9.28465377e-01 -2.76795928e-01  4.49288985e-02\n",
      "  -5.15482162e-01 -2.76486517e-01 -2.31869318e-01 -2.18014247e-01\n",
      "   1.67424199e-01 -1.06413874e-01 -4.15036990e-01  1.97334745e-01\n",
      "  -2.82301615e-01  9.81787412e-01 -1.64536088e-01 -2.82436282e-01\n",
      "  -2.42155731e-01 -2.18422711e-01  8.89073877e-01  3.18606814e-01\n",
      "  -3.29278164e-01 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "  -2.16846891e-01 -1.87066926e-01 -1.53955194e-01  7.11510539e-01\n",
      "   3.93936846e-01 -1.47276334e-01 -2.99669671e-01]\n",
      " [ 7.56080555e-02 -1.02139832e+00  2.16702584e-01 -2.82481970e-01\n",
      "   9.51372985e-01 -1.85285591e-01 -3.05085509e-02 -9.13550989e-01\n",
      "   1.21485047e-01 -2.72775869e-01  9.37176205e-01 -1.60052500e-01\n",
      "  -9.83897521e-01 -9.73845743e-01 -2.76486517e-01  3.54203568e-02\n",
      "  -5.25742815e-01  2.07657793e-01 -2.82436282e-01  9.74503276e-01\n",
      "  -1.73682360e-01 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "   8.72924624e-01  3.12718157e-01 -3.33026302e-01 -1.27939880e-01\n",
      "  -6.82620797e-01 -5.71284869e-03 -2.50596845e-01  7.45250694e-01\n",
      "  -1.68712050e-01 -1.00026769e+00 -8.63211955e-01 -2.72613642e-01\n",
      "   1.14914361e-01 -4.86648361e-01  1.48370403e-01 -2.72554353e-01\n",
      "   9.53995173e-01 -1.49953687e-01 -2.72775869e-01 -2.41189285e-01\n",
      "  -2.14023406e-01  8.94175180e-01  3.27537540e-01 -3.11093464e-01\n",
      "  -8.71419185e-01 -9.70897416e-01 -2.64101662e-01 -4.02686649e-01\n",
      "  -6.93399191e-01 -9.28465377e-01 -2.76795928e-01  4.49288985e-02\n",
      "  -5.15482162e-01 -2.76486517e-01 -2.31869318e-01 -2.18014247e-01\n",
      "   1.67424199e-01 -1.06413874e-01 -4.15036990e-01  1.97334745e-01\n",
      "  -2.82301615e-01  9.81787412e-01 -1.64536088e-01 -2.82436282e-01\n",
      "  -2.42155731e-01 -2.18422711e-01  8.89073877e-01  3.18606814e-01\n",
      "  -3.29278164e-01 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "  -2.16846891e-01 -1.87066926e-01 -1.53955194e-01  7.11510539e-01\n",
      "   3.93936846e-01 -1.47276334e-01 -2.99669671e-01]\n",
      " [ 5.56938779e-02 -9.66078095e-01  2.14722985e-01 -2.82481970e-01\n",
      "   1.08633158e+00  5.82973567e-01 -5.09873634e-02 -8.65329434e-01\n",
      "   1.01710336e-01 -2.72775869e-01  1.05530043e+00  5.46184325e-01\n",
      "  -9.47931712e-01 -9.20183784e-01 -2.76486517e-01  1.48710923e-01\n",
      "  -4.64230483e-03  2.05496396e-01 -2.82436282e-01  1.10969933e+00\n",
      "   5.90625385e-01 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "   1.07792260e+00  1.25958067e+00  2.14011230e-01 -1.47744558e-01\n",
      "  -6.53326055e-01 -2.62626394e-02 -2.50596845e-01  8.34733148e-01\n",
      "   4.07976154e-01 -9.62265655e-01 -8.16874653e-01 -2.72613642e-01\n",
      "   2.32995037e-01  3.98627976e-02  1.29326910e-01 -2.72554353e-01\n",
      "   1.07183019e+00  5.53608567e-01 -2.72775869e-01 -2.41189285e-01\n",
      "  -2.14023406e-01  1.08952355e+00  1.26936130e+00  1.97513515e-01\n",
      "  -8.50751647e-01 -9.35218405e-01 -2.64101662e-01 -3.34980044e-01\n",
      "  -4.01908742e-01 -8.76708185e-01 -2.76795928e-01  1.57504020e-01\n",
      "   1.64873336e-03 -2.76486517e-01 -2.31869318e-01 -2.18014247e-01\n",
      "   3.16978723e-01  5.17657934e-01 -3.09168206e-02  1.94991938e-01\n",
      "  -2.82301615e-01  1.11573515e+00  5.93480520e-01 -2.82436282e-01\n",
      "  -2.42155731e-01 -2.18422711e-01  1.09484354e+00  1.26106921e+00\n",
      "   2.15946174e-01 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "  -2.16846891e-01 -1.87066926e-01 -1.53955194e-01  9.61975095e-01\n",
      "   1.33489273e+00  6.15856736e-01 -2.62872407e-02]\n",
      " [ 1.15436411e-01 -1.02139832e+00  2.16702584e-01 -2.82481970e-01\n",
      "   1.29840937e+00  1.60731911e+00  1.07055938e-02 -8.96447777e-01\n",
      "   1.60108681e-01 -2.72775869e-01  1.29304802e+00  1.52870285e+00\n",
      "  -9.83897521e-01 -9.73845743e-01 -2.76486517e-01  2.21039288e-01\n",
      "   6.09565501e-01  2.07657793e-01 -2.82436282e-01  1.32286651e+00\n",
      "   1.61015958e+00 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "   1.42077125e+00  2.69537721e+00  1.35736895e+00 -8.78326462e-02\n",
      "  -6.57126283e-01  3.51553682e-02 -2.50596845e-01  1.06248336e+00\n",
      "   1.24446007e+00 -9.94168921e-01 -8.46704933e-01 -2.72613642e-01\n",
      "   3.34502871e-01  6.88387036e-01  1.84715145e-01 -2.72554353e-01\n",
      "   1.30979217e+00  1.53284034e+00 -2.72775869e-01 -2.41189285e-01\n",
      "  -2.14023406e-01  1.46086699e+00  2.75205708e+00  1.28671569e+00\n",
      "  -8.71419185e-01 -9.70897416e-01 -2.64101662e-01 -3.21885540e-01\n",
      "  -9.88399410e-02 -9.28465377e-01 -2.76795928e-01  2.29624291e-01\n",
      "   6.11454835e-01 -2.76486517e-01 -2.31869318e-01 -2.18014247e-01\n",
      "   4.76809843e-01  1.36678065e+00  7.21764764e-01  1.97334745e-01\n",
      "  -2.82301615e-01  1.32764652e+00  1.60507638e+00 -2.82436282e-01\n",
      "  -2.42155731e-01 -2.18422711e-01  1.43956284e+00  2.69075865e+00\n",
      "   1.35578262e+00 -2.82481970e-01 -2.42002063e-01 -2.18590970e-01\n",
      "  -2.16846891e-01 -1.87066926e-01 -1.53955194e-01  1.40790246e+00\n",
      "   2.93508193e+00  2.37554542e+00  8.64620927e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train[:5][:5])\n",
    "z_score(x_train)\n",
    "print(x_train[:5][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a753ca8-d202-4acb-b13b-749e32f57d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute cost\n",
    "\n",
    "def compute_cost(x,y,w,b,lambda_):\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(w,x[i]) + b\n",
    "        #print(f_wb)\n",
    "        cost += (f_wb - y[i])**2\n",
    "        # print(cost)\n",
    "    reg_cost = 0                            #Regularization\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j])**2\n",
    "    reg_cost *= (lambda_/(2*m))\n",
    "    J_wb = (cost/(2*m)) + reg_cost                   #Total cost\n",
    "    return J_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b0cd645-f0b3-4ee4-88d0-82d3b67ad1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[669.7845196]\n"
     ]
    }
   ],
   "source": [
    "# Testing compute cost\n",
    "lambda_ = 0\n",
    "w = np.zeros(x_train.shape[1],)\n",
    "b = 0.0\n",
    "print(compute_cost(x_train, y_train, w, b,lambda_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d9c9a16-90a5-442e-b832-46490f269548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw = w - alpha*dJ/dw\\nb = b - alpha*dJ/db\\n\\ndJ/dw = 1/m sumof(i in range(m)) (f_wb - y[i])x_i_j\\ndJ/db = 1/m sumof(i in range(m)) (f_wb - y[i])\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Failed attempt at gradient descent\n",
    "\n",
    "'''\n",
    "w = w - alpha*dJ/dw\n",
    "b = b - alpha*dJ/db\n",
    "\n",
    "dJ/dw = 1/m sumof(i in range(m)) (f_wb - y[i])x_i_j\n",
    "dJ/db = 1/m sumof(i in range(m)) (f_wb - y[i])\n",
    "'''\n",
    "# def gd(x,y,w,alpha):\n",
    "# for j in range(n):\n",
    "#     for i in range(m):\n",
    "#         f_wb = np.dot(w,x[i])+b\n",
    "#         w[j] = w[j] - (alpha * (f_wb - y[i]) * x[i][j])\n",
    "#         b = b - (alpha * (f_wb - y[i]))\n",
    "# ^^^Absolutely preposterous. Change it completely. Whatever simultaneous update means^^^\n",
    "# If you can't crack it, check out multiple linear regression lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa45466-4b5f-4a77-a559-d96ee03d664e",
   "metadata": {},
   "source": [
    "Gradient descent with regularization:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})  $$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$\n",
    "\n",
    "Compare this to the gradient of the cost function without regularization:\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a58cd75-9bfb-4d2d-b594-5665b8443974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Notes:\\nf(x) = w1x1 + w2x2 + w3x3 + ...... + b.  That means b is one and the same for the f(x). It's not different for each feature. So you don't have to \\nput it in a loop; it'll come outside.\\n\\nAlso, f_wb and y[i] are both numpy arrays. So when you subtract them, it results in an array too. And then you're trying to assign it to a scalar \\nonly spot (numpy array slots are scalar only unless you assign it otherwise) while it is a vector (a matrix/array). This might throw an error so grab \\nthe value inside dj_dw and use *that* in the operation. (Strangely, this doesn't affect dj_db because it is assigned to a variable, not a scalar only \\nspot like the np array.)\\n\\nAlso, if you look Coursera's implementation, they have done dj_dw = np.zeros((n,)). This will create an array for dj_dw something like this \\n[0.0, 0.0, 0.0]. And after that they'll assign w from this array of dj_dw which makes complete sense too. But, what I'm doing is that I'm updating \\nonly the first element of dj_dw and instantly assigning it to temp_w. That way I don't have to create an array.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining gradient descent\n",
    "\n",
    "def gd(x,y,w,b,alpha,lambda_):\n",
    "    temp_w = np.zeros(n)\n",
    "    dj_db = 0\n",
    "    for j in range(n):\n",
    "        dj_dw = 0\n",
    "        for i in range(m):\n",
    "            f_wb = np.dot(w,x[i]) + b\n",
    "            dj_dw += ((f_wb - y[i])*x[i][j])\n",
    "            dj_db += (f_wb - y[i])\n",
    "        dj_dw/=m\n",
    "        dj_dw += (lambda_/m)*w[j]                      # gradient descent for regularization\n",
    "        temp_w[j] = w[j] - alpha * dj_dw[0]            # (You can perhaps update it directly without needing a temp var?)\n",
    "        w[j] = temp_w[j]\n",
    "    dj_db/=m\n",
    "    temp_b = b - alpha * dj_db\n",
    "    b = temp_b\n",
    "\n",
    "    return w,b\n",
    "\n",
    "'''Notes:\n",
    "f(x) = w1x1 + w2x2 + w3x3 + ...... + b.  That means b is one and the same for the f(x). It's not different for each feature. So you don't have to \n",
    "put it in a loop; it'll come outside.\n",
    "\n",
    "Also, f_wb and y[i] are both numpy arrays. So when you subtract them, it results in an array too. And then you're trying to assign it to a scalar \n",
    "only spot (numpy array slots are scalar only unless you assign it otherwise) while it is a vector (a matrix/array). This might throw an error so grab \n",
    "the value inside dj_dw and use *that* in the operation. (Strangely, this doesn't affect dj_db because it is assigned to a variable, not a scalar only \n",
    "spot like the np array.)\n",
    "\n",
    "Also, if you look Coursera's implementation, they have done dj_dw = np.zeros((n,)). This will create an array for dj_dw something like this \n",
    "[0.0, 0.0, 0.0]. And after that they'll assign w from this array of dj_dw which makes complete sense too. But, what I'm doing is that I'm updating \n",
    "only the first element of dj_dw and instantly assigning it to temp_w. That way I don't have to create an array.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44739b2c-2dff-4edc-82c9-0e58c970f17f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: [306.40820648]\n"
     ]
    }
   ],
   "source": [
    "# Training model (by computing cost and doing gradient descent repeatedly)\n",
    "\n",
    "start_time = time.time()\n",
    "w = np.zeros(n, dtype=float)\n",
    "b = 0.0\n",
    "alpha = 0.02\n",
    "lambda_ = 0.01\n",
    "J_history = []\n",
    "iterations = 1500\n",
    "\n",
    "for i in range(iterations+1):\n",
    "    w,b = gd(x_train,y_train,w,b,alpha,lambda_)\n",
    "\n",
    "    if i%100 == 0:\n",
    "        cost = compute_cost(x_train,y_train,w,b,lambda_)\n",
    "        J_history.append(cost[0])\n",
    "        print(f\"Cost after {i} iterations: {cost}\")\n",
    "        if len(J_history)>1:\n",
    "            if J_history[-2]-J_history[-1] < 0.01: # The current cost - the previous cost\n",
    "                break\n",
    "            \n",
    "end_time = time.time()\n",
    "# print(\"Cost history: \",J_history)\n",
    "duration = end_time - start_time\n",
    "print(f\"Time elapsed: {round(duration//60)} minutes {round(duration%60)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a800e15-25b3-4cbc-9ce5-7ddc99b23cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks GPT -_- . Plotting J_history\n",
    "plt.plot(J_history)\n",
    "plt.xlabel(\"Iteration (per 100)\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost Convergence\")\n",
    "plt.show()\n",
    "plt.plot(J_history[1:])\n",
    "plt.xlabel(\"Iteration (per 100)\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost Convergence after first 100 iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01270a93-fd8b-4acc-aa4f-10f9fcd8dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c60e9-a04a-4632-a3d0-3e90189bf173",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = add_interaction_terms(x_test_raw)\n",
    "z_score(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdc3f6-5273-4d17-b2eb-ab1c922b338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(m,x):\n",
    "    predicted = np.zeros(m,dtype=float)\n",
    "    for i in range(m):\n",
    "        predicted[i] = np.dot(w,x[i]) + b[0] # Did b[0] cuz b has shape (1,). Can't assign vector to a scalar spot.\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1942f39-7332-4031-b533-3144156936cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an array of predicted values\n",
    "\n",
    "predicted_train = predict(m,x_train) # Prediction based on training data\n",
    "\n",
    "m_test = x_test.shape[0]\n",
    "predicted_test = predict(m_test,x_test) # Prediction based on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40737e73-f754-4f3e-b0f2-00ef2e592a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining R^2\n",
    "\n",
    "# R^2 = 1-(SumOfSquares_Total/SumOfSquares_Residual)\n",
    "# SumOfSquares_Total = Summation ((target - mean(target))^2)\n",
    "# SumOfSquares_Residual = Summation ((target - predicted)^2)\n",
    "\n",
    "def r_square(target,predicted,m):\n",
    "    sum_sst = 0\n",
    "    sum_ssr = 0\n",
    "    mean = np.mean(target,axis=0)\n",
    "    for i in range(m):\n",
    "        sum_sst += (target[i] - mean)**2\n",
    "        sum_ssr += (target[i] - predicted[i])**2\n",
    "    \n",
    "    r_sq = 1 - (sum_ssr/sum_sst)\n",
    "    return r_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5d050-eedc-4f3b-93c9-4205b77487bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining RMSE\n",
    "\n",
    "# Root mean squared error = sqrt(summation[1-N](predicted[i] - actual[i])^2/N)\n",
    "\n",
    "# def rmse(target,predicted):\n",
    "#     sum = 0\n",
    "#     for i in range(m):\n",
    "#         sum += (predicted[i] - target[i])^2\n",
    "#     result = (sum/m)**0.5\n",
    "#     return result\n",
    "\n",
    "# Better way\n",
    "\n",
    "def rmse(target,predicted):\n",
    "    return np.sqrt(np.mean((predicted - target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af520382-140b-4f9c-863f-9a343f584011",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data\")\n",
    "print(\"-------------\")\n",
    "print(f\"R square: {r_square(y_train,predicted_train,m)}\")\n",
    "print(f\"RMSE: {rmse(y_train,predicted_train)}\")\n",
    "print(f\"NRMSE: {rmse(y_train,predicted_train)/(np.max(y_train)-np.min(y_train))}\")\n",
    "print()\n",
    "print(\"Testing Data\")\n",
    "print(\"------------\")\n",
    "print(f\"R square: {r_square(y_test,predicted_test,m_test)}\")\n",
    "print(f\"RMSE: {rmse(y_test,predicted_test)}\")\n",
    "print(f\"NRMSE: {rmse(y_test,predicted_test)/(np.max(y_test)-np.min(y_test))}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0f622ed-0a65-4047-98ff-b69cbc650513",
   "metadata": {},
   "source": [
    "Notes\n",
    "-----\n",
    "\n",
    "RuntimeWarning: overflow encountered in add\n",
    "  dj_dw += ((f_wb - y[i])*x[i][j])\n",
    " RuntimeWarning: invalid value encountered in dot\n",
    "  f_wb = np.dot(w,x[i]) + b\n",
    "\n",
    "Alllrriiight. This what raw ML training looks like. Building linear regression from scratch and encountering errors because w blew up. My guy GPT \n",
    "suggested this would be a good time to try out reducing alpha and perhaps trying out normalizing the parameters. So that's what I'm gonna do...\n",
    "\n",
    "[1] The first thing I did is reduce alpha from 0.1 to 0.001. It was much slower than the previous run and still it resulted in the same error. Sooo,\n",
    "now I'm planning to start trying out Z-score normalization.\n",
    "\n",
    "- Z-score normalization is called standardization; it centers the data around 0 with std dev as 1. min-max thing is called normalization; It normalizes\n",
    "the data from 0 to 1.\n",
    "- If normalization also doesn't work, it miight be a case of over or under-fitting. That's a whole 'nother process.\n",
    "\n",
    "[2] Ok, standardized x_train and ran it again. But it didn't work for some reason. Turns out I didnt even call zscore() lol. But even after that it \n",
    "didn't work. I figured it might be because of the boolean column in the array. Turned it to float to maintain uniformity. Still didn't work. Turns out \n",
    "np.mean() and np.std() don't work when x_train's datatype is object. It should be float (it wasn't because there was a column of int). So there are two\n",
    "options: (1) Use your own definition for std_dev or (2) Convert the int column to float and use np.std()\n",
    "\n",
    "[3] Fine, so I tried using my own definition of std_dev (It is commented out in the cell above the z_score definition). It worked, but all the elements\n",
    "in the array were printed out like this np.float64(0.09552223315050891). It was wrapping all the elements in float64 because it considered x_train as \n",
    "an object type still (which is apparently bad). So the remaining option is to make it uniform and convert the int column into float. Ok, so that too \n",
    "din't work. So instead of converting individual columns to floats, I converted the entire x_train to float and now its dtype is float64, not object.\n",
    "\n",
    "- x_train was of object type because it had columns of different data types (dtypes). So I had to convert it all into float to make it uniform. If I \n",
    "used my own std_dev function definition, it could still work, but it would remain as an object. The problem with that is when running operations on it \n",
    "could potentially crash. Also, it could possibly store something like 'dog' where it should only be float. Because it doesn't recognize it as a float\n",
    "type and instead as an object.\n",
    "- float64 is the same as float. In numpy it specifically mentions the 64, whereas in python it doesn't. It essentially is a high precision decimal \n",
    "number. Again, the same as python's float (from what I understood)\n",
    "\n",
    "[4] YES! It works. Super slow, but still works. Reduced cost greatly from ~660 to ~19. Levelled off at 19. But I wanna decrease it even more. Gonna do \n",
    "a couple of things to analyze it. See how much time it takes to run the entire training process (just interested. Not necessary). Plot a graph using \n",
    "matplotlib to see if it is over or underfitting. If required, proceed to regularization or polynomial regression (hope it doesn't go to that).\n",
    "\n",
    "[5] Added time count. Compare costs before and after removing wind speed and wind directions. (I feel they're pretty \n",
    "useless as it is proved by its w; around 0.2 which is v less). Also add graphs later to visualize it better.\n",
    "Ran the first round of testing. It WORKS! It's not accurate, but it works. Gotta work out the kinks now...not sure what to improve. Have a few ideas \n",
    "though. Here were the results --> (1) Predicted temp: 32.42732953 Actual temp: 37.2 (2) Predicted: 32.50329364 Actual: 37.6\n",
    "After removing wind direction and speed (1) Predicted: 32.36955401 Actual: 37.2 (2) 32.40309452 Actual: 37.6\n",
    "\n",
    "[6] So same as before. Try plotting graphs and polynomial regression. You probably can keep the wind features (slightly lesser error). Maybe separate \n",
    "the training part and testing part into 2 files. As a final step ig use MAE and RMSE to calculate error.\n",
    "\n",
    "[7] Graph is a mess. The error is quite big. Not sure how I should proceed. Probably try cleaning the data. Cut into half ig? Split it into two: \n",
    "training and testing.\n",
    "\n",
    "[8] Did the exact same thing using scikit learn in literal minutes, instead of hours of learning to implement cost_computing, gradient_descent, and \n",
    "training the model. Even the training barely took a couple of seconds! As compared to my model which took more than 10 minutes. In any case, the end \n",
    "result is more or less the same: The same messy graph. It could be bittersweet - Good because it shows that my model training was done the right way. \n",
    "Bad because the model is messed up in both cases. It should be because the data is noisy. Right?\n",
    "\n",
    "[9] Post interaction with Zarif. The graph is an absolute blunder.. don't know why I didn't catch it earlier. It makes no sense. An example of where \n",
    "ChatGPT can go wrong. What do the x and y coordinates mean and what does the corresponding point indicate? Again no sense. Learned that you can't plot \n",
    "a simple graph for these kinda cases where there are many features. So I'm planning to rely on R^2 evaluation. Also, ima try making it quadratic \n",
    "regression.\n",
    "\n",
    "- R^2 for linear regression: 0.6150321\n",
    "- R^2 for quadratic regression: 0.62093213\n",
    "\n",
    "[10] Ok that is pretty good. It might look like a small improvement, but I did see the cost decrease a lot too. Took literally 43 whole minute man! \n",
    "Anyway, this prediction is of the training data only. I gotta try it on testing data too and compare the results with linear. And then I might even \n",
    "try doing cubic regression. Don't know if it is necessary though, or if it would lead to overfitting. Another thing to consider is the way I'm \n",
    "selecting the training and testing data. Currently I'm choosing alternating data, i.e. all the even rows for training and all the odd rows for testing.\n",
    "Is that a good way or should I do it via some other method? Hmm... Gotta see. Also maybe try regularization. And maybe even try playing around by \n",
    "removing a feature or two and seeing how it works.\n",
    "\n",
    "[11] Still haven't done regularization but will do soon. As for the training/testing, I'm unsure if I'm training it too much. Have to experiment with \n",
    "different datasets. 1) 2018-2023, 2) Half (alternating data) from 2018-2023 3) 2021-2023, 4) Half (alternatig data) from 2021-2023. Testing data will \n",
    "be data from 2024 (acquired via huggingface <- reddit) [Remember to remove LasVegas. It's completely new data, not pre-trained]. And then there are \n",
    "external factors that affect the model that are not included in the training data. For example track resurfacing. Data is probably going to change \n",
    "significantly. See if you have to or don't have to exclude those tracks too.\n",
    "\n",
    "[12] Z-score doesnt work in adgp2023. AAARGHH! Shoots NaN values because 0 division I guess. What I learnt from this project? The importance of having good clean data. Now gotta make z-score fail-safe\n",
    "\n",
    "[13] Successfully made z-score fail-safe. Now focusing on testing the accuracy of the model using r_sq and rmse on the testing data for a change. Mind exploded when it showed this -> R square: [-1.09962984e+11]; RMSE: 2811277.6260124487; NRMSE: 89531.13458638372\n",
    "Haha GPT pointed out that I might not have normalized the test data, which in fact I hadn't. I forgot lol. So going to do that now, and then see if it is good. If the values are high, I might have to implement regularization. At this point, I feel like I'm nearing the end of the project. The cost hasn't gone down significantly and it's not exactly very accurate either. But then again, things like these weather stuff aren't meant to be put into regression models. They deserve something more complex. Besides this is my first regression project, and so considering that, my project miight be a litttle overkill. In any case, there was some significant progress along the journey and it was a good journey altogether. So respect for myself.\n",
    "\n",
    "[14] Ugh, yet another issue. But that's the real process. No regret. Just be more vigilant, not careless with your code. add_interaction_terms is causing problems. It took almost a 1000 seconds to do it for x_test, and then I realized that instead of creating 27 interaction features, it made 82620. Crazyy. The problem was that it was a function specifically designed for x_train. Wrong move. Violates the S.O.L.I.D. principles and didn't work for testing data. Genralized it and now it works\n",
    "\n",
    "[15] Training Data\n",
    "-------------\n",
    "R square: [0.63906851]\n",
    "RMSE: 12.053371972101697\n",
    "NRMSE: 0.22656714233273867\n",
    "\n",
    "Testing Data\n",
    "------------\n",
    "R square: [0.59875803]\n",
    "RMSE: 11.239201110783089\n",
    "NRMSE: 0.3579363411077417\n",
    "\n",
    "Is this any good? Maybe... For starters it's not overfitting. I might still go ahead with regularization, just holding on to a string of hope to reduce the cost further. I even considered doing cubic, but it's quite a hassle to change things up and see it overfit anyway. ChatGPT suggested against it too. (Edit: I hate it when it contradicts itself. Now it's saying that it's underfitting and cubic might help. Blud can be annoying at times). The cost could have gone down further... only if I had more features - cloud cover, rainfall measurement, time of day, etc. But that is one of the most rarest resources: data. The thing that is most abundant in the world, yet when you want it, the most difficult to get. Based on the data that I have to work with, I think I've made pretty good progress. Only thing left; regularization. Here I come.\n",
    "\n",
    "[16] Updated humidity to whole numbers to match testing data. Implemented regularization. Result: No difference. So I'm setting lambda back to 0 (turning off regularization cuz it's not much use) and calling it a day; not a successful one, but not a failure either. Might go ahead with cubic as a last resort, but if that too doesn't yield any promising results, I'm gonna end this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
